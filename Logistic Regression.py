# -*- coding: utf-8 -*-
"""Research_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1deGRBrdeXJ1QqydAHXluEjcXpWbrVFZs

**Connect to the Google drive**
"""

# prompt: connect to the google drive

from google.colab import drive
drive.mount('/content/drive')

"""**Import required libraries**"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import MinMaxScaler
from scipy.stats import skew
from sklearn.model_selection import train_test_split

"""**Read the dataset**

Features:

1. Age: Customer's age
2. Gender: Customer's gender (0: Male, 1: Female)
3. Annual Income: Annual income of the customer in dollars
4. Number of Purchases: Total number of purchases made by the customer
5. Product Category: Category of the purchased product (0: Electronics, 1: Clothing, 2: Home Goods, 3: Beauty, 4: Sports)
6. Time Spent on Website: Time spent by the customer on the website in minutes
7. Loyalty Program: Whether the customer is a member of the loyalty program (0: No, 1: Yes)
8. Discounts Availed: Number of discounts availed by the customer (range: 0-5)
9. PurchaseStatus (Target Variable): Likelihood of the customer making a purchase (0: No, 1: Yes)
"""

df = pd.read_csv('/content/drive/MyDrive/DataSet/customer_purchase_data.csv')

df.head()

df.tail()

"""**Characteristic of the Data set**
(sanity check of data)
"""

df.shape

df.info()

#Finding the missing values

df.isnull().sum()

#Finding the duplicate values
df.duplicated().sum()

"""**Exploratory Data Analysis**"""

#Descriptive Statistics
df.describe().T

#Histogram for Identify the distribution of each column
for i in df.select_dtypes(include='number').columns:
  sns.histplot(data=df,x=i,kde = True)
  plt.show()

# Box-plot to identify the outliers
for i in df.select_dtypes(include='number').columns:
  sns.boxplot(data=df,x=i)
  plt.show()

df.select_dtypes(include='number').columns

"""Histogram for each variable group with the target variable"""

relation_purches = df[['Age', 'Gender', 'AnnualIncome', 'NumberOfPurchases', 'ProductCategory',
       'TimeSpentOnWebsite', 'LoyaltyProgram', 'DiscountsAvailed']]

for column in relation_purches.columns:

    sns.histplot(data = df ,x=column,hue=df['PurchaseStatus'],  multiple='dodge')
    plt.title(f'Distribution of {column} by PurchaseStatus')
    plt.show()

"""Customer Distribution by product category"""

product_counts = df['ProductCategory'].value_counts()

product_counts.index = product_counts.index.map({0: 'Electronics', 1: 'Clothing', 2: 'Home Goods', 3: 'Beauty', 4: 'Sports'})

product_counts.plot(kind='pie', autopct='%1.2f%%')

plt.ylabel('')
plt.title('Customer Distribution by Product Category')
plt.show()

"""Correlation between Variables"""

corr_matrix = df.corr()

plt.figure(figsize=(8, 4))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""1. The heatmap shows the relationship between different variables, indicating where strong or weak correlations exist.
2. Variables like LoyaltyProgram, DiscountsAvailed, and TimeSpentOnWebsite seem to have a more significant positive relationship with PurchaseStatus, while Age has a negative relationship.
3. Understanding these correlations can help identify which factors most strongly influence purchasing behavior in the dataset.
"""

#Scatter plot for understand relationship between target variable and other independent variables
for i in ['Age', 'Gender', 'AnnualIncome', 'NumberOfPurchases', 'ProductCategory',
       'TimeSpentOnWebsite', 'LoyaltyProgram', 'DiscountsAvailed']:
  sns.scatterplot(data=df,x=i,y='PurchaseStatus')
  plt.show()

"""This scatterplot shows no specific pattern is identified.

# Data Preprossecing Techniques
"""

#Drop the duplicate values
df =df.drop_duplicates()

#Drop the missing values(There is no missing values in the dataset)
df = df.dropna()

df.info()

df.head()

df.tail()

df = df.reset_index(drop=True)

#Outlier Treatment(There is no outliers in the dataset)
#only for the continuous numerrical columns. Do not do categorical variable and the target variable
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
Outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))
df_cleaned = df[~Outliers.any(axis=1)]

df_cleaned.info()

"""**Normalied the data set**"""

continuous_features = ['Age', 'AnnualIncome', 'NumberOfPurchases','TimeSpentOnWebsite', 'DiscountsAvailed']
min_max_scaler = MinMaxScaler()
df_cleaned[continuous_features] = min_max_scaler.fit_transform(df_cleaned[continuous_features])
print(df_cleaned.head())

# Find the skewness of the countinous variables. If skewness >1 or <1 those are
# highly skewed distributions no need to transformation data
for feature in continuous_features:
    skewness = skew(df_cleaned[feature])
    print(f'{feature} Skewness: {skewness}')

"""**Check for the Imbalance of the dataset**:
Handling imbalanced datasets is crucial when building predictive models, especially for binary classification tasks like predicting "PurchaseStatus". If one class is significantly underrepresented, the model might become biased towards the majority class, leading to poor performance in predicting the minority class. Here are several methods to handle imbalanced datasets:
"""

# Alternatively, get the value counts
print(df_cleaned['PurchaseStatus'].value_counts(normalize=True))  # Shows proportion of each class

"""**Split the data into train and the test**"""

X= df_cleaned.drop(columns=['PurchaseStatus'])
y=df_cleaned['PurchaseStatus']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
print(df_cleaned.shape)

"""**Logistic Regression Model**"""

# Step 1: Import necessary libraries
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Step 2: Initialize the logistic regression model
logreg = LogisticRegression(random_state=42)

# Step 3: Train the model using the training data
logreg.fit(X_train, y_train)

# Step 4: Make predictions on the test data
y_pred = logreg.predict(X_test)

# Step 5: Evaluate the model
# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(conf_matrix)

# Classification Report
class_report = classification_report(y_test, y_pred)
print('Classification Report:')
print(class_report)

# prompt: Make prediction with model with nice interface for enter to the data

# Create a simple input interface
print("Enter customer details:")
age = float(input("Age: "))
gender = int(input("Gender (0: Male, 1: Female): "))
annual_income = float(input("Annual Income: "))
num_purchases = float(input("Number of Purchases: "))
product_category = int(input("Product Category (0: Electronics, 1: Clothing, 2: Home Goods, 3: Beauty, 4: Sports): "))
time_spent = float(input("Time Spent on Website (minutes): "))
loyalty = int(input("Loyalty Program Member (0: No, 1: Yes): "))
discounts = float(input("Discounts Availed: "))

# Create a NumPy array with the input data for continuous variables only
input_data_continuous = np.array([[age, annual_income, num_purchases, time_spent, discounts]])

# Normalize the continuous input data using the same scaler used for training
input_data_normalized = min_max_scaler.transform(input_data_continuous)

# Create the full input data including categorical variables
input_data = np.array([[input_data_normalized[0][0], gender, input_data_normalized[0][1], input_data_normalized[0][2],
                        product_category, input_data_normalized[0][3], loyalty, input_data_normalized[0][4]]])

# Make a prediction
prediction = logreg.predict(input_data)

# Display the prediction
if prediction[0] == 0:
    print("Prediction: Not Likely to Purchase")
else:
    print("Prediction: Likely to Purchase")